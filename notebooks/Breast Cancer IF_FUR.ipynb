{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy import sparse\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from grad_utils import grad_logloss_theta_lr\n",
    "from grad_utils import batch_grad_logloss_lr\n",
    "from inverse_hvp import inverse_hvp_lr_newtonCG\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import pdb\n",
    "import os\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score for the model on test set = 0.9298245614035088\n",
      "Prediction model type = classification\n",
      "Number of categorical features = 0\n",
      "IF Calculation\n",
      "R2 score for the model on test set = 0.9298245614035088\n",
      "x:  [[ 1.18217814e+01  2.64321305e+01  9.87667089e+01 ...  2.63774017e-02\n",
      "   2.08455306e-01  5.83785289e-02]\n",
      " [ 1.10425720e+01  2.91227595e+01  6.88868702e+01 ...  1.60109995e-02\n",
      "   2.34939752e-01  7.24014737e-02]\n",
      " [ 1.40542534e+01  3.21220821e+01  5.33892329e+01 ... -4.09096725e-02\n",
      "   2.54049884e-01  8.42364403e-02]\n",
      " ...\n",
      " [ 1.41887435e+01  2.62403047e+01  8.52257400e+01 ...  1.40376219e-03\n",
      "   1.97240773e-01  5.83898485e-02]\n",
      " [ 1.43382160e+01  3.15855146e+01  9.77381831e+01 ...  3.88440291e-02\n",
      "   2.51838724e-01  6.00950074e-02]\n",
      " [ 1.22378322e+01  3.15293903e+01  8.26411366e+01 ...  4.41269870e-02\n",
      "   2.58408748e-01  6.53583358e-02]]\n",
      "y_x:  [1 0 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1\n",
      " 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 0 0\n",
      " 1 1 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 0 1 0 1 0 1 1 0 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 0 1 1 0\n",
      " 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0 1\n",
      " 1 0 1 1 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 0 0 1 0 0 1 0\n",
      " 1 0 0 1 0 0 0 1 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0\n",
      " 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 1 1 0 0\n",
      " 0 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0\n",
      " 1 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 1 1 1 1 0 0 0 1\n",
      " 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 0 0 1 1\n",
      " 0 1 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0\n",
      " 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1\n",
      " 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 0 1 1\n",
      " 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1 0 1 0 1 1 1 0\n",
      " 1 1 1 1 1 0 0 0 1 0 0 0 1 1 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 0 0 0 1 0 1 1\n",
      " 1 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1\n",
      " 1 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 1\n",
      " 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1\n",
      " 0 0 0 1 0 0 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 1\n",
      " 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 0 1 1 0 1\n",
      " 1 1 0 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 1 0 0 0 1 1 1 0 1 0 0 1 1\n",
      " 1 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 1 1 1 0 1 0 1 0 0 0 0 1 1 1\n",
      " 0 1 1 1 0 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 0 1 0 1\n",
      " 0]\n",
      "X_init:  [[1.227e+01 2.997e+01 7.742e+01 4.654e+02 7.699e-02 3.398e-02 0.000e+00\n",
      "  0.000e+00 1.701e-01 5.960e-02 4.455e-01 3.647e+00 2.884e+00 3.513e+01\n",
      "  7.339e-03 8.243e-03 0.000e+00 0.000e+00 3.141e-02 3.136e-03 1.345e+01\n",
      "  3.805e+01 8.508e+01 5.589e+02 9.422e-02 5.213e-02 0.000e+00 0.000e+00\n",
      "  2.409e-01 6.743e-02]]\n",
      "y_x0:  [1]\n",
      "Succeed in getting the inverse of preconditioner M.\n",
      "iter 0 cg iter 0 iter_diff 537.3790757893985\n",
      "iter 1 cg iter 0 iter_diff 1.7527804491668157\n",
      "iter 2 cg iter 0 iter_diff 0.8427558375961866\n",
      "iter 3 cg iter 0 iter_diff 0.28863035103443224\n",
      "iter 4 cg iter 0 iter_diff 0.029867687242486327\n",
      "iter 5 cg iter 0 iter_diff 0.003355974247727487\n",
      "iter 5 cg iter 10 iter_diff 0.0007573746453833753\n",
      "iter 6 cg iter 0 iter_diff 0.00013942357607052365\n",
      "iter 7 cg iter 0 iter_diff 0.00015875046395985714\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.000041\n",
      "         Iterations: 8\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 16\n",
      "         Hessian evaluations: 36\n",
      "IF VALUE:  -0.09696366730754623\n",
      "f_acqu:  [[-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]\n",
      " [-9999.99996838]]\n",
      "IF Calculation\n",
      "R2 score for the model on test set = 0.9298245614035088\n",
      "x:  [[ 1.18217814e+01  2.64321305e+01  9.87667089e+01  7.55764894e+02\n",
      "   8.58205541e-02  7.13254569e-02  3.06305817e-02  2.16069794e-04\n",
      "   1.79379440e-01  6.26353438e-02  2.70063800e-01  3.49768473e+00\n",
      "   1.67954979e+00  4.15272053e+01  7.60916376e-03 -2.07792594e-03\n",
      "  -2.28959394e-02 -9.07230768e-04  3.86354901e-02  4.57326273e-03\n",
      "   1.28657002e+01  4.23771700e+01  6.48567637e+01  4.55395759e+02\n",
      "   9.97356992e-02 -1.07763704e-01  9.48538877e-02  2.63774017e-02\n",
      "   2.08455306e-01  5.83785289e-02]]\n",
      "y_x:  [1]\n",
      "X_init:  [[1.227e+01 2.997e+01 7.742e+01 4.654e+02 7.699e-02 3.398e-02 0.000e+00\n",
      "  0.000e+00 1.701e-01 5.960e-02 4.455e-01 3.647e+00 2.884e+00 3.513e+01\n",
      "  7.339e-03 8.243e-03 0.000e+00 0.000e+00 3.141e-02 3.136e-03 1.345e+01\n",
      "  3.805e+01 8.508e+01 5.589e+02 9.422e-02 5.213e-02 0.000e+00 0.000e+00\n",
      "  2.409e-01 6.743e-02]]\n",
      "y_x0:  [1]\n",
      "Succeed in getting the inverse of preconditioner M.\n",
      "iter 0 cg iter 0 iter_diff 537.3790757893985\n",
      "iter 1 cg iter 0 iter_diff 1.7527804491668135\n",
      "iter 2 cg iter 0 iter_diff 0.8427558375961719\n",
      "iter 3 cg iter 0 iter_diff 0.28863036468917935\n",
      "iter 4 cg iter 0 iter_diff 0.029867663974993224\n",
      "iter 5 cg iter 0 iter_diff 0.003560139944726169\n",
      "iter 5 cg iter 10 iter_diff 0.0005361910719717557\n",
      "iter 6 cg iter 0 iter_diff 0.000139281257008655\n",
      "iter 7 cg iter 0 iter_diff 2.8074693984458153e-05\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.000041\n",
      "         Iterations: 8\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 16\n",
      "         Hessian evaluations: 43\n",
      "IF VALUE:  -0.09608073384373887\n",
      "f_acqu:  [[-9999.99996838]]\n",
      "IF Calculation\n",
      "R2 score for the model on test set = 0.9298245614035088\n",
      "x:  [[ 1.18217814e+01  2.64321305e+01  9.87667089e+01  7.55764894e+02\n",
      "   8.58205541e-02  7.13254569e-02  3.06305817e-02  2.16069794e-04\n",
      "   1.79379440e-01  6.26353438e-02  2.70063800e-01  3.49768473e+00\n",
      "   1.67954979e+00  4.15272053e+01  7.60916376e-03 -2.07792594e-03\n",
      "  -2.28959394e-02 -9.07230768e-04  3.86354901e-02  4.57326273e-03\n",
      "   1.28657002e+01  4.23771700e+01  6.48567637e+01  4.55395759e+02\n",
      "   9.97356992e-02 -1.07763704e-01  9.48538877e-02  2.63774017e-02\n",
      "   2.08455306e-01  5.83785289e-02]]\n",
      "y_x:  [1]\n",
      "X_init:  [[1.227e+01 2.997e+01 7.742e+01 4.654e+02 7.699e-02 3.398e-02 0.000e+00\n",
      "  0.000e+00 1.701e-01 5.960e-02 4.455e-01 3.647e+00 2.884e+00 3.513e+01\n",
      "  7.339e-03 8.243e-03 0.000e+00 0.000e+00 3.141e-02 3.136e-03 1.345e+01\n",
      "  3.805e+01 8.508e+01 5.589e+02 9.422e-02 5.213e-02 0.000e+00 0.000e+00\n",
      "  2.409e-01 6.743e-02]]\n",
      "y_x0:  [1]\n",
      "Succeed in getting the inverse of preconditioner M.\n",
      "iter 0 cg iter 0 iter_diff 537.3790757893985\n",
      "iter 1 cg iter 0 iter_diff 1.7527804491668135\n",
      "iter 2 cg iter 0 iter_diff 0.8427558375961719\n",
      "iter 3 cg iter 0 iter_diff 0.28863036468917935\n",
      "iter 4 cg iter 0 iter_diff 0.029867663974993224\n",
      "iter 5 cg iter 0 iter_diff 0.003560139944726169\n",
      "iter 5 cg iter 10 iter_diff 0.0005361910719717557\n",
      "iter 6 cg iter 0 iter_diff 0.000139281257008655\n",
      "iter 7 cg iter 0 iter_diff 2.8074693984458153e-05\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.000041\n",
      "         Iterations: 8\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 16\n",
      "         Hessian evaluations: 43\n",
      "IF VALUE:  -0.09608073384373887\n",
      "f_acqu:  [[-9999.99996838]]\n",
      "IF Calculation\n",
      "R2 score for the model on test set = 0.9298245614035088\n",
      "x:  [[ 1.18217814e+01  2.64321305e+01  9.87667089e+01  7.55764894e+02\n",
      "   8.58205541e-02  7.13254569e-02  3.06305817e-02  2.16069794e-04\n",
      "   1.79379440e-01  6.26353438e-02  2.70063800e-01  3.49768473e+00\n",
      "   1.67954979e+00  4.15272053e+01  7.60916376e-03 -2.07792594e-03\n",
      "  -2.28959394e-02 -9.07230768e-04  3.86354901e-02  4.57326273e-03\n",
      "   1.28657002e+01  4.23771700e+01  6.48567637e+01  4.55395759e+02\n",
      "   9.97356992e-02 -1.07763704e-01  9.48538877e-02  2.63774017e-02\n",
      "   2.08455306e-01  5.83785289e-02]]\n",
      "y_x:  [1]\n",
      "X_init:  [[1.227e+01 2.997e+01 7.742e+01 4.654e+02 7.699e-02 3.398e-02 0.000e+00\n",
      "  0.000e+00 1.701e-01 5.960e-02 4.455e-01 3.647e+00 2.884e+00 3.513e+01\n",
      "  7.339e-03 8.243e-03 0.000e+00 0.000e+00 3.141e-02 3.136e-03 1.345e+01\n",
      "  3.805e+01 8.508e+01 5.589e+02 9.422e-02 5.213e-02 0.000e+00 0.000e+00\n",
      "  2.409e-01 6.743e-02]]\n",
      "y_x0:  [1]\n",
      "Succeed in getting the inverse of preconditioner M.\n",
      "iter 0 cg iter 0 iter_diff 537.3790757893985\n",
      "iter 1 cg iter 0 iter_diff 1.7527804491668135\n",
      "iter 2 cg iter 0 iter_diff 0.8427558375961719\n",
      "iter 3 cg iter 0 iter_diff 0.28863036468917935\n",
      "iter 4 cg iter 0 iter_diff 0.029867663974993224\n",
      "iter 5 cg iter 0 iter_diff 0.003560139944726169\n",
      "iter 5 cg iter 10 iter_diff 0.0005361910719717557\n",
      "iter 6 cg iter 0 iter_diff 0.000139281257008655\n",
      "iter 7 cg iter 0 iter_diff 2.8074693984458153e-05\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.000041\n",
      "         Iterations: 8\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 16\n",
      "         Hessian evaluations: 43\n",
      "IF VALUE:  -0.09608073384373887\n",
      "f_acqu:  [[-9999.99996838]]\n",
      "IF Calculation\n",
      "R2 score for the model on test set = 0.9298245614035088\n",
      "x:  [[ 1.18217814e+01  2.64321305e+01  9.87667089e+01  7.55764894e+02\n",
      "   8.58205541e-02  7.13254569e-02  3.06305817e-02  2.16069794e-04\n",
      "   1.79379440e-01  6.26353438e-02  2.70063800e-01  3.49768473e+00\n",
      "   1.67954979e+00  4.15272053e+01  7.60916376e-03 -2.07792594e-03\n",
      "  -2.28959394e-02 -9.07230768e-04  3.86354901e-02  4.57326273e-03\n",
      "   1.28657002e+01  4.23771700e+01  6.48567637e+01  4.55395759e+02\n",
      "   9.97356992e-02 -1.07763704e-01  9.48538877e-02  2.63774017e-02\n",
      "   2.08455306e-01  5.83785289e-02]]\n",
      "y_x:  [1]\n",
      "X_init:  [[1.227e+01 2.997e+01 7.742e+01 4.654e+02 7.699e-02 3.398e-02 0.000e+00\n",
      "  0.000e+00 1.701e-01 5.960e-02 4.455e-01 3.647e+00 2.884e+00 3.513e+01\n",
      "  7.339e-03 8.243e-03 0.000e+00 0.000e+00 3.141e-02 3.136e-03 1.345e+01\n",
      "  3.805e+01 8.508e+01 5.589e+02 9.422e-02 5.213e-02 0.000e+00 0.000e+00\n",
      "  2.409e-01 6.743e-02]]\n",
      "y_x0:  [1]\n",
      "Succeed in getting the inverse of preconditioner M.\n",
      "iter 0 cg iter 0 iter_diff 537.3790757893985\n",
      "iter 1 cg iter 0 iter_diff 1.7527804491668135\n",
      "iter 2 cg iter 0 iter_diff 0.8427558375961719\n",
      "iter 3 cg iter 0 iter_diff 0.28863036468917935\n",
      "iter 4 cg iter 0 iter_diff 0.029867663974993224\n",
      "iter 5 cg iter 0 iter_diff 0.003560139944726169\n",
      "iter 5 cg iter 10 iter_diff 0.0005361910719717557\n",
      "iter 6 cg iter 0 iter_diff 0.000139281257008655\n",
      "iter 7 cg iter 0 iter_diff 2.8074693984458153e-05\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.000041\n",
      "         Iterations: 8\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 16\n",
      "         Hessian evaluations: 43\n",
      "IF VALUE:  -0.09608073384373887\n",
      "f_acqu:  [[-9999.99996838]]\n",
      "IF Calculation\n",
      "R2 score for the model on test set = 0.9298245614035088\n",
      "x:  [[ 1.18217814e+01  2.64321305e+01  9.87667089e+01  7.55764894e+02\n",
      "   8.58205541e-02  7.13254569e-02  3.06305817e-02  2.16069794e-04\n",
      "   1.79379440e-01  6.26353438e-02  2.70063800e-01  3.49768473e+00\n",
      "   1.67954979e+00  4.15272053e+01  7.60916376e-03 -2.07792594e-03\n",
      "  -2.28959394e-02 -9.07230768e-04  3.86354901e-02  4.57326273e-03\n",
      "   1.28657002e+01  4.23771700e+01  6.48567637e+01  4.55395759e+02\n",
      "   9.97356992e-02 -1.07763704e-01  9.48538877e-02  2.63774017e-02\n",
      "   2.08455306e-01  5.83785289e-02]]\n",
      "y_x:  [1]\n",
      "X_init:  [[1.227e+01 2.997e+01 7.742e+01 4.654e+02 7.699e-02 3.398e-02 0.000e+00\n",
      "  0.000e+00 1.701e-01 5.960e-02 4.455e-01 3.647e+00 2.884e+00 3.513e+01\n",
      "  7.339e-03 8.243e-03 0.000e+00 0.000e+00 3.141e-02 3.136e-03 1.345e+01\n",
      "  3.805e+01 8.508e+01 5.589e+02 9.422e-02 5.213e-02 0.000e+00 0.000e+00\n",
      "  2.409e-01 6.743e-02]]\n",
      "y_x0:  [1]\n",
      "Succeed in getting the inverse of preconditioner M.\n",
      "iter 0 cg iter 0 iter_diff 537.3790757893985\n",
      "iter 1 cg iter 0 iter_diff 1.7527804491668135\n",
      "iter 2 cg iter 0 iter_diff 0.8427558375961719\n",
      "iter 3 cg iter 0 iter_diff 0.28863036468917935\n",
      "iter 4 cg iter 0 iter_diff 0.029867663974993224\n",
      "iter 5 cg iter 0 iter_diff 0.003560139944726169\n",
      "iter 5 cg iter 10 iter_diff 0.0005361910719717557\n",
      "iter 6 cg iter 0 iter_diff 0.000139281257008655\n",
      "iter 7 cg iter 0 iter_diff 2.8074693984458153e-05\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.000041\n",
      "         Iterations: 8\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 16\n",
      "         Hessian evaluations: 43\n",
      "IF VALUE:  -0.09608073384373887\n",
      "f_acqu:  [[-9999.99996838]]\n",
      "IF Calculation\n",
      "R2 score for the model on test set = 0.9298245614035088\n",
      "x:  [[ 1.18217814e+01  2.64321305e+01  9.87667089e+01  7.55764894e+02\n",
      "   8.58205641e-02  7.13254569e-02  3.06305817e-02  2.16069794e-04\n",
      "   1.79379440e-01  6.26353438e-02  2.70063800e-01  3.49768473e+00\n",
      "   1.67954979e+00  4.15272053e+01  7.60916376e-03 -2.07792594e-03\n",
      "  -2.28959394e-02 -9.07230768e-04  3.86354901e-02  4.57326273e-03\n",
      "   1.28657002e+01  4.23771700e+01  6.48567637e+01  4.55395759e+02\n",
      "   9.97356992e-02 -1.07763704e-01  9.48538877e-02  2.63774017e-02\n",
      "   2.08455306e-01  5.83785289e-02]]\n",
      "y_x:  [1]\n",
      "X_init:  [[1.227e+01 2.997e+01 7.742e+01 4.654e+02 7.699e-02 3.398e-02 0.000e+00\n",
      "  0.000e+00 1.701e-01 5.960e-02 4.455e-01 3.647e+00 2.884e+00 3.513e+01\n",
      "  7.339e-03 8.243e-03 0.000e+00 0.000e+00 3.141e-02 3.136e-03 1.345e+01\n",
      "  3.805e+01 8.508e+01 5.589e+02 9.422e-02 5.213e-02 0.000e+00 0.000e+00\n",
      "  2.409e-01 6.743e-02]]\n",
      "y_x0:  [1]\n",
      "Succeed in getting the inverse of preconditioner M.\n",
      "iter 0 cg iter 0 iter_diff 537.3790757893985\n",
      "iter 1 cg iter 0 iter_diff 1.7527804491668135\n",
      "iter 2 cg iter 0 iter_diff 0.8427558375961719\n",
      "iter 3 cg iter 0 iter_diff 0.28863036468917935\n",
      "iter 4 cg iter 0 iter_diff 0.029867663974993224\n",
      "iter 5 cg iter 0 iter_diff 0.003560139944726169\n",
      "iter 5 cg iter 10 iter_diff 0.0005361910719717557\n",
      "iter 6 cg iter 0 iter_diff 0.000139281257008655\n",
      "iter 7 cg iter 0 iter_diff 2.8074693984458153e-05\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.000041\n",
      "         Iterations: 8\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 16\n",
      "         Hessian evaluations: 43\n",
      "IF VALUE:  -0.09608073384373887\n",
      "f_acqu:  [[-9999.99996838]]\n",
      "IF Calculation\n",
      "R2 score for the model on test set = 0.9298245614035088\n",
      "x:  [[ 1.18217814e+01  2.64321305e+01  9.87667089e+01  7.55764894e+02\n",
      "   8.58205541e-02  7.13254669e-02  3.06305817e-02  2.16069794e-04\n",
      "   1.79379440e-01  6.26353438e-02  2.70063800e-01  3.49768473e+00\n",
      "   1.67954979e+00  4.15272053e+01  7.60916376e-03 -2.07792594e-03\n",
      "  -2.28959394e-02 -9.07230768e-04  3.86354901e-02  4.57326273e-03\n",
      "   1.28657002e+01  4.23771700e+01  6.48567637e+01  4.55395759e+02\n",
      "   9.97356992e-02 -1.07763704e-01  9.48538877e-02  2.63774017e-02\n",
      "   2.08455306e-01  5.83785289e-02]]\n",
      "y_x:  [1]\n",
      "X_init:  [[1.227e+01 2.997e+01 7.742e+01 4.654e+02 7.699e-02 3.398e-02 0.000e+00\n",
      "  0.000e+00 1.701e-01 5.960e-02 4.455e-01 3.647e+00 2.884e+00 3.513e+01\n",
      "  7.339e-03 8.243e-03 0.000e+00 0.000e+00 3.141e-02 3.136e-03 1.345e+01\n",
      "  3.805e+01 8.508e+01 5.589e+02 9.422e-02 5.213e-02 0.000e+00 0.000e+00\n",
      "  2.409e-01 6.743e-02]]\n",
      "y_x0:  [1]\n",
      "Succeed in getting the inverse of preconditioner M.\n",
      "iter 0 cg iter 0 iter_diff 537.3790757893985\n",
      "iter 1 cg iter 0 iter_diff 1.7527804491668135\n",
      "iter 2 cg iter 0 iter_diff 0.8427558375961719\n",
      "iter 3 cg iter 0 iter_diff 0.28863036468917935\n",
      "iter 4 cg iter 0 iter_diff 0.029867663974993224\n",
      "iter 5 cg iter 0 iter_diff 0.003560139944726169\n",
      "iter 5 cg iter 10 iter_diff 0.0005361910719717557\n",
      "iter 6 cg iter 0 iter_diff 0.000139281257008655\n",
      "iter 7 cg iter 0 iter_diff 2.8074693984458153e-05\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.000041\n",
      "         Iterations: 8\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 16\n",
      "         Hessian evaluations: 43\n",
      "IF VALUE:  -0.09608073384373887\n",
      "f_acqu:  [[-9999.99996838]]\n",
      "IF Calculation\n",
      "R2 score for the model on test set = 0.9298245614035088\n",
      "x:  [[ 1.18217814e+01  2.64321305e+01  9.87667089e+01  7.55764894e+02\n",
      "   8.58205541e-02  7.13254569e-02  3.06305917e-02  2.16069794e-04\n",
      "   1.79379440e-01  6.26353438e-02  2.70063800e-01  3.49768473e+00\n",
      "   1.67954979e+00  4.15272053e+01  7.60916376e-03 -2.07792594e-03\n",
      "  -2.28959394e-02 -9.07230768e-04  3.86354901e-02  4.57326273e-03\n",
      "   1.28657002e+01  4.23771700e+01  6.48567637e+01  4.55395759e+02\n",
      "   9.97356992e-02 -1.07763704e-01  9.48538877e-02  2.63774017e-02\n",
      "   2.08455306e-01  5.83785289e-02]]\n",
      "y_x:  [1]\n",
      "X_init:  [[1.227e+01 2.997e+01 7.742e+01 4.654e+02 7.699e-02 3.398e-02 0.000e+00\n",
      "  0.000e+00 1.701e-01 5.960e-02 4.455e-01 3.647e+00 2.884e+00 3.513e+01\n",
      "  7.339e-03 8.243e-03 0.000e+00 0.000e+00 3.141e-02 3.136e-03 1.345e+01\n",
      "  3.805e+01 8.508e+01 5.589e+02 9.422e-02 5.213e-02 0.000e+00 0.000e+00\n",
      "  2.409e-01 6.743e-02]]\n",
      "y_x0:  [1]\n",
      "Succeed in getting the inverse of preconditioner M.\n",
      "iter 0 cg iter 0 iter_diff 537.3790757893985\n",
      "iter 1 cg iter 0 iter_diff 1.7527804491668135\n",
      "iter 2 cg iter 0 iter_diff 0.8427558375961719\n",
      "iter 3 cg iter 0 iter_diff 0.28863036468917935\n",
      "iter 4 cg iter 0 iter_diff 0.029867663974993224\n",
      "iter 5 cg iter 0 iter_diff 0.003560139944726169\n",
      "iter 5 cg iter 10 iter_diff 0.0005361910719717557\n",
      "iter 6 cg iter 0 iter_diff 0.000139281257008655\n",
      "iter 7 cg iter 0 iter_diff 2.8074693984458153e-05\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.000041\n",
      "         Iterations: 8\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 16\n",
      "         Hessian evaluations: 43\n",
      "IF VALUE:  -0.09608073384373887\n",
      "f_acqu:  [[-9999.99996838]]\n",
      "IF Calculation\n",
      "R2 score for the model on test set = 0.9298245614035088\n",
      "x:  [[ 1.18217814e+01  2.64321305e+01  9.87667089e+01  7.55764894e+02\n",
      "   8.58205541e-02  7.13254569e-02  3.06305817e-02  2.16079794e-04\n",
      "   1.79379440e-01  6.26353438e-02  2.70063800e-01  3.49768473e+00\n",
      "   1.67954979e+00  4.15272053e+01  7.60916376e-03 -2.07792594e-03\n",
      "  -2.28959394e-02 -9.07230768e-04  3.86354901e-02  4.57326273e-03\n",
      "   1.28657002e+01  4.23771700e+01  6.48567637e+01  4.55395759e+02\n",
      "   9.97356992e-02 -1.07763704e-01  9.48538877e-02  2.63774017e-02\n",
      "   2.08455306e-01  5.83785289e-02]]\n",
      "y_x:  [1]\n",
      "X_init:  [[1.227e+01 2.997e+01 7.742e+01 4.654e+02 7.699e-02 3.398e-02 0.000e+00\n",
      "  0.000e+00 1.701e-01 5.960e-02 4.455e-01 3.647e+00 2.884e+00 3.513e+01\n",
      "  7.339e-03 8.243e-03 0.000e+00 0.000e+00 3.141e-02 3.136e-03 1.345e+01\n",
      "  3.805e+01 8.508e+01 5.589e+02 9.422e-02 5.213e-02 0.000e+00 0.000e+00\n",
      "  2.409e-01 6.743e-02]]\n",
      "y_x0:  [1]\n",
      "Succeed in getting the inverse of preconditioner M.\n",
      "iter 0 cg iter 0 iter_diff 537.3790757893985\n",
      "iter 1 cg iter 0 iter_diff 1.7527804491668135\n",
      "iter 2 cg iter 0 iter_diff 0.8427558375961719\n",
      "iter 3 cg iter 0 iter_diff 0.28863036468917935\n",
      "iter 4 cg iter 0 iter_diff 0.029867663974993224\n",
      "iter 5 cg iter 0 iter_diff 0.003560139944726169\n",
      "iter 5 cg iter 10 iter_diff 0.0005361910719717557\n",
      "iter 6 cg iter 0 iter_diff 0.000139281257008655\n",
      "iter 7 cg iter 0 iter_diff 2.8074693984458153e-05\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.000041\n",
      "         Iterations: 8\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 16\n",
      "         Hessian evaluations: 43\n",
      "IF VALUE:  -0.09608073384373887\n",
      "f_acqu:  [[-9999.99996838]]\n",
      "IF Calculation\n",
      "R2 score for the model on test set = 0.9298245614035088\n",
      "x:  [[ 1.18217814e+01  2.64321305e+01  9.87667089e+01  7.55764894e+02\n",
      "   8.58205541e-02  7.13254569e-02  3.06305817e-02  2.16069794e-04\n",
      "   1.79379450e-01  6.26353438e-02  2.70063800e-01  3.49768473e+00\n",
      "   1.67954979e+00  4.15272053e+01  7.60916376e-03 -2.07792594e-03\n",
      "  -2.28959394e-02 -9.07230768e-04  3.86354901e-02  4.57326273e-03\n",
      "   1.28657002e+01  4.23771700e+01  6.48567637e+01  4.55395759e+02\n",
      "   9.97356992e-02 -1.07763704e-01  9.48538877e-02  2.63774017e-02\n",
      "   2.08455306e-01  5.83785289e-02]]\n",
      "y_x:  [1]\n",
      "X_init:  [[1.227e+01 2.997e+01 7.742e+01 4.654e+02 7.699e-02 3.398e-02 0.000e+00\n",
      "  0.000e+00 1.701e-01 5.960e-02 4.455e-01 3.647e+00 2.884e+00 3.513e+01\n",
      "  7.339e-03 8.243e-03 0.000e+00 0.000e+00 3.141e-02 3.136e-03 1.345e+01\n",
      "  3.805e+01 8.508e+01 5.589e+02 9.422e-02 5.213e-02 0.000e+00 0.000e+00\n",
      "  2.409e-01 6.743e-02]]\n",
      "y_x0:  [1]\n",
      "Succeed in getting the inverse of preconditioner M.\n",
      "iter 0 cg iter 0 iter_diff 537.3790757893985\n",
      "iter 1 cg iter 0 iter_diff 1.7527804491668135\n",
      "iter 2 cg iter 0 iter_diff 0.8427558375961719\n",
      "iter 3 cg iter 0 iter_diff 0.28863036468917935\n",
      "iter 4 cg iter 0 iter_diff 0.029867663974993224\n",
      "iter 5 cg iter 0 iter_diff 0.003560139944726169\n",
      "iter 5 cg iter 10 iter_diff 0.0005361910719717557\n",
      "iter 6 cg iter 0 iter_diff 0.000139281257008655\n",
      "iter 7 cg iter 0 iter_diff 2.8074693984458153e-05\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.000041\n",
      "         Iterations: 8\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 16\n",
      "         Hessian evaluations: 43\n",
      "IF VALUE:  -0.09608073384373887\n",
      "f_acqu:  [[-9999.99996838]]\n",
      "IF Calculation\n",
      "R2 score for the model on test set = 0.9298245614035088\n",
      "x:  [[ 1.18217814e+01  2.64321305e+01  9.87667089e+01  7.55764894e+02\n",
      "   8.58205541e-02  7.13254569e-02  3.06305817e-02  2.16069794e-04\n",
      "   1.79379440e-01  6.26353538e-02  2.70063800e-01  3.49768473e+00\n",
      "   1.67954979e+00  4.15272053e+01  7.60916376e-03 -2.07792594e-03\n",
      "  -2.28959394e-02 -9.07230768e-04  3.86354901e-02  4.57326273e-03\n",
      "   1.28657002e+01  4.23771700e+01  6.48567637e+01  4.55395759e+02\n",
      "   9.97356992e-02 -1.07763704e-01  9.48538877e-02  2.63774017e-02\n",
      "   2.08455306e-01  5.83785289e-02]]\n",
      "y_x:  [1]\n",
      "X_init:  [[1.227e+01 2.997e+01 7.742e+01 4.654e+02 7.699e-02 3.398e-02 0.000e+00\n",
      "  0.000e+00 1.701e-01 5.960e-02 4.455e-01 3.647e+00 2.884e+00 3.513e+01\n",
      "  7.339e-03 8.243e-03 0.000e+00 0.000e+00 3.141e-02 3.136e-03 1.345e+01\n",
      "  3.805e+01 8.508e+01 5.589e+02 9.422e-02 5.213e-02 0.000e+00 0.000e+00\n",
      "  2.409e-01 6.743e-02]]\n",
      "y_x0:  [1]\n",
      "Succeed in getting the inverse of preconditioner M.\n",
      "iter 0 cg iter 0 iter_diff 537.3790757893985\n",
      "iter 1 cg iter 0 iter_diff 1.7527804491668135\n",
      "iter 2 cg iter 0 iter_diff 0.8427558375961719\n",
      "iter 3 cg iter 0 iter_diff 0.28863036468917935\n",
      "iter 4 cg iter 0 iter_diff 0.029867663974993224\n",
      "iter 5 cg iter 0 iter_diff 0.003560139944726169\n",
      "iter 5 cg iter 10 iter_diff 0.0005361910719717557\n",
      "iter 6 cg iter 0 iter_diff 0.000139281257008655\n",
      "iter 7 cg iter 0 iter_diff 2.8074693984458153e-05\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.000041\n",
      "         Iterations: 8\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 16\n",
      "         Hessian evaluations: 43\n",
      "IF VALUE:  -0.09608073384373887\n",
      "f_acqu:  [[-9999.99996838]]\n",
      "IF Calculation\n",
      "R2 score for the model on test set = 0.9298245614035088\n",
      "x:  [[ 1.18217814e+01  2.64321305e+01  9.87667089e+01  7.55764894e+02\n",
      "   8.58205541e-02  7.13254569e-02  3.06305817e-02  2.16069794e-04\n",
      "   1.79379440e-01  6.26353438e-02  2.70063810e-01  3.49768473e+00\n",
      "   1.67954979e+00  4.15272053e+01  7.60916376e-03 -2.07792594e-03\n",
      "  -2.28959394e-02 -9.07230768e-04  3.86354901e-02  4.57326273e-03\n",
      "   1.28657002e+01  4.23771700e+01  6.48567637e+01  4.55395759e+02\n",
      "   9.97356992e-02 -1.07763704e-01  9.48538877e-02  2.63774017e-02\n",
      "   2.08455306e-01  5.83785289e-02]]\n",
      "y_x:  [1]\n",
      "X_init:  [[1.227e+01 2.997e+01 7.742e+01 4.654e+02 7.699e-02 3.398e-02 0.000e+00\n",
      "  0.000e+00 1.701e-01 5.960e-02 4.455e-01 3.647e+00 2.884e+00 3.513e+01\n",
      "  7.339e-03 8.243e-03 0.000e+00 0.000e+00 3.141e-02 3.136e-03 1.345e+01\n",
      "  3.805e+01 8.508e+01 5.589e+02 9.422e-02 5.213e-02 0.000e+00 0.000e+00\n",
      "  2.409e-01 6.743e-02]]\n",
      "y_x0:  [1]\n",
      "Succeed in getting the inverse of preconditioner M.\n",
      "iter 0 cg iter 0 iter_diff 537.3790757893985\n",
      "iter 1 cg iter 0 iter_diff 1.7527804491668135\n",
      "iter 2 cg iter 0 iter_diff 0.8427558375961719\n",
      "iter 3 cg iter 0 iter_diff 0.28863036468917935\n",
      "iter 4 cg iter 0 iter_diff 0.029867663974993224\n",
      "iter 5 cg iter 0 iter_diff 0.003560139944726169\n",
      "iter 5 cg iter 10 iter_diff 0.0005361910719717557\n",
      "iter 6 cg iter 0 iter_diff 0.000139281257008655\n",
      "iter 7 cg iter 0 iter_diff 2.8074693984458153e-05\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.000041\n",
      "         Iterations: 8\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 16\n",
      "         Hessian evaluations: 43\n",
      "IF VALUE:  -0.09608073384373887\n",
      "f_acqu:  [[-9999.99996838]]\n",
      "IF Calculation\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "\n",
    "from evaluation.blackbox_util import BlackBoxSimulator\n",
    "dataset_utilities = BlackBoxSimulator().load_breast_cancer_utilities()\n",
    "[X_train, y_train, X_test, y_test, features, model, mode, categorical_features, sample_idx] = dataset_utilities.values()\n",
    "\n",
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "assert X_test.shape[0] == y_test.shape[0]\n",
    "\n",
    "print(f\"Prediction model type = {mode}\")\n",
    "print(f\"Number of categorical features = {len(categorical_features)}\")\n",
    "\n",
    "from unravel.tabular import UnRAVELTabularExplainer\n",
    "sample_idx = sample_idx[0]\n",
    "X_init = np.array([X_test[sample_idx]])\n",
    "f_e = UnRAVELTabularExplainer(model, X_train, categorical_features = categorical_features, mode = mode)\n",
    "\n",
    "explanation_IF_FUR = f_e.explain(X_init = X_init, feature_names = features, alpha = \"IF_FUR\", max_iter = 1, importance_method=\"ARD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<StemContainer object of 3 artists>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.stem(explanation_IF_FUR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IF CALCULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[1.26017814e+01, 1.50521305e+01, 1.06436709e+02, 8.02364894e+02,\n",
    "  1.17030554e-01, 1.67745457e-01, 1.26660582e-01, 5.62460698e-02,\n",
    "  2.12779440e-01, 6.80453438e-02, 1.35163800e-01, 1.36068473e+00,\n",
    "  1.38554979e+00, 2.79672053e+01, 8.07716376e-03, 2.89990741e-02,\n",
    "  2.82240606e-02, 1.78527692e-02, 3.58254901e-02, 7.15226273e-03,\n",
    "  1.36057002e+01, 2.91771700e+01, 7.39967637e+01, 4.87695759e+02,\n",
    "  1.39815699e-01, 1.05906296e-01, 3.52153888e-01, 1.52177402e-01,\n",
    "  2.78855306e-01, 7.41185289e-02]]\n",
    "\n",
    "x = np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_init = [[1.305e+01, 1.859e+01, 8.509e+01, 5.120e+02, 1.082e-01, 1.304e-01, 9.603e-02,\n",
    "  5.603e-02, 2.035e-01, 6.501e-02, 3.106e-01, 1.510e+00, 2.590e+00, 2.157e+01,\n",
    "  7.807e-03, 3.932e-02, 5.112e-02, 1.876e-02, 2.860e-02, 5.715e-03, 1.419e+01,\n",
    "  2.485e+01, 9.422e+01, 5.912e+02, 1.343e-01, 2.658e-01, 2.573e-01, 1.258e-01,\n",
    "  3.113e-01, 8.317e-02]]\n",
    "\n",
    "X_init = np.array(X_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Jugaadu IF\n",
      "R2 score for the model on test set = 0.9298245614035088\n",
      "X_train shape:  (462, 30)\n",
      "y_train shape:  (462,)\n",
      "X_test shape:  (57, 30)\n",
      "y_test shape:  (57,)\n",
      "x_va shape:  (50, 30)\n",
      "y_va shape:  (50,)\n",
      "X_train_orig shape:  (463, 30)\n",
      "y_train_orig shape:  (463,)\n",
      "X_train_pert shape:  (463, 30)\n",
      "y_train_pert shape:  (463,)\n",
      "tr_pred_orig shape:  (463,)\n",
      "tr_pred_pert shape:  (463,)\n",
      "Succeed in getting the inverse of preconditioner M.\n",
      "iter 0 cg iter 0 iter_diff 869.5501115345966\n",
      "iter 1 cg iter 0 iter_diff 13.935906432028306\n",
      "iter 2 cg iter 0 iter_diff 1.3671158605779765\n",
      "iter 3 cg iter 0 iter_diff 0.3394364461087338\n",
      "iter 4 cg iter 0 iter_diff 0.15992462923518352\n",
      "iter 5 cg iter 0 iter_diff 0.05689910803994071\n",
      "iter 6 cg iter 0 iter_diff 0.004866914500374307\n",
      "iter 7 cg iter 0 iter_diff 0.00017409401819629857\n",
      "iter 7 cg iter 10 iter_diff 1.5864103373236646e-05\n",
      "iter 8 cg iter 0 iter_diff 1.5864103373149548e-05\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.000030\n",
      "         Iterations: 9\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 18\n",
      "         Hessian evaluations: 39\n",
      "Succeed in getting the inverse of preconditioner M.\n",
      "iter 0 cg iter 0 iter_diff 869.5501115345966\n",
      "iter 1 cg iter 0 iter_diff 13.935906432028306\n",
      "iter 2 cg iter 0 iter_diff 1.3671158605779765\n",
      "iter 3 cg iter 0 iter_diff 0.3394364461087338\n",
      "iter 4 cg iter 0 iter_diff 0.15992462923518352\n",
      "iter 5 cg iter 0 iter_diff 0.05689910803994071\n",
      "iter 6 cg iter 0 iter_diff 0.004866914500374307\n",
      "iter 7 cg iter 0 iter_diff 0.00017409401819629857\n",
      "iter 7 cg iter 10 iter_diff 1.5864103373236646e-05\n",
      "iter 8 cg iter 0 iter_diff 1.5864103373149548e-05\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.000030\n",
      "         Iterations: 9\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 18\n",
      "         Hessian evaluations: 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pnans/Documents/GitHub/UnRAvEL/inverse_hvp.py:142: OptimizeWarning: Unknown solver options: preconditioner\n",
      "  fmin_results = fmin_ncg(f=fmin_loss_fn,\n",
      "/home/pnans/Documents/GitHub/UnRAvEL/inverse_hvp.py:142: OptimizeWarning: Unknown solver options: preconditioner\n",
      "  fmin_results = fmin_ncg(f=fmin_loss_fn,\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import pdb\n",
    "import os\n",
    "np.random.seed(0)\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "import sys\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "\n",
    "sys.path.append('..')\n",
    "from grad_utils import grad_logloss_theta_lr\n",
    "from grad_utils import batch_grad_logloss_lr\n",
    "from inverse_hvp import inverse_hvp_lr_newtonCG\n",
    "\n",
    "print(\"Computing Jugaadu IF\")\n",
    "\n",
    "from evaluation.blackbox_util import BlackBoxSimulator\n",
    "dataset_utilities = BlackBoxSimulator().load_breast_cancer_utilities()\n",
    "[X_train, y_train, X_test, y_test, features, model, mode, categorical_features, sample_idx] = dataset_utilities.values()\n",
    "\n",
    "x_va = X_train[-50:]\n",
    "y_va = y_train[-50:]\n",
    "X_train = X_train[:512-50]\n",
    "y_train = y_train[:512-50]\n",
    "\n",
    "sigmoid_k = 10\n",
    "C = 0.1\n",
    "sample_ratio = 0.6\n",
    "flip_ratio = 0.4\n",
    "num_tr_sample = X_train.shape[0]\n",
    "obj_sample_size = int(sample_ratio * num_tr_sample)\n",
    "\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"y_test shape: \", y_test.shape)\n",
    "print(\"x_va shape: \", x_va.shape)\n",
    "print(\"y_va shape: \", y_va.shape)\n",
    "\n",
    "\n",
    "clf = LogisticRegression(\n",
    "        C = C,\n",
    "        fit_intercept=False,\n",
    "        tol = 1e-8,\n",
    "        solver=\"liblinear\",\n",
    "        multi_class=\"ovr\",\n",
    "        max_iter=100,\n",
    "        warm_start=False,\n",
    "        verbose=0,\n",
    "        )\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "y_va_pred = clf.predict_proba(x_va)[:,1]\n",
    "full_logloss = log_loss(y_va,y_va_pred)\n",
    "weight_ar = clf.coef_.flatten()\n",
    "y_te_pred = clf.predict_proba(X_test)[:,1]\n",
    "full_te_logloss = log_loss(y_test,y_te_pred)\n",
    "full_te_auc = roc_auc_score(y_test, y_te_pred)\n",
    "y_te_pred = clf.predict(X_test)\n",
    "full_te_acc = (y_test == y_te_pred).sum() / y_test.shape[0]\n",
    "\n",
    "if_start_time = time.time()\n",
    "test_grad_loss_val = grad_logloss_theta_lr(y_va,y_va_pred,x_va,weight_ar,C,False,0.1/(num_tr_sample*C))\n",
    "tr_pred = clf.predict_proba(X_train)[:,1]\n",
    "batch_size = 10000\n",
    "M = None\n",
    "total_batch = int(np.ceil(num_tr_sample / float(batch_size)))\n",
    "for idx in range(total_batch):\n",
    "    batch_tr_grad = batch_grad_logloss_lr(y_train[idx*batch_size:(idx+1)*batch_size],\n",
    "        tr_pred[idx*batch_size:(idx+1)*batch_size],\n",
    "        X_train[idx*batch_size:(idx+1)*batch_size],\n",
    "        weight_ar,\n",
    "        C,\n",
    "        False,\n",
    "        1.0)\n",
    "\n",
    "    sum_grad = batch_tr_grad.multiply(X_train[idx*batch_size:(idx+1)*batch_size]).sum(0)\n",
    "    if M is None:\n",
    "        M = sum_grad\n",
    "    else:\n",
    "        M = M + sum_grad\n",
    "        \n",
    "M = M + 0.1/(num_tr_sample*C) * np.ones(X_train.shape[1])\n",
    "M = np.array(M).flatten()\n",
    "\n",
    "\n",
    "y_x = clf.predict(x)\n",
    "y_x0 = clf.predict(X_init)\n",
    "\n",
    "X_train_orig = np.concatenate((X_train, X_init))\n",
    "y_train_orig = np.concatenate((y_train, y_x0))\n",
    "X_train_pert = np.concatenate((X_train, x))\n",
    "y_train_pert = np.concatenate((y_train, y_x))\n",
    "tr_pred_orig = np.concatenate((tr_pred, y_x0))\n",
    "tr_pred_pert = np.concatenate((tr_pred, y_x))\n",
    "\n",
    "\n",
    "print(\"X_train_orig shape: \", X_train_orig.shape)\n",
    "print(\"y_train_orig shape: \", y_train_orig.shape)\n",
    "print(\"X_train_pert shape: \", X_train_pert.shape)\n",
    "print(\"y_train_pert shape: \", y_train_pert.shape)\n",
    "print(\"tr_pred_orig shape: \", tr_pred_orig.shape)\n",
    "print(\"tr_pred_pert shape: \", tr_pred_pert.shape)\n",
    "\n",
    "\n",
    "iv_hvp_orig = inverse_hvp_lr_newtonCG(X_train_orig,y_train_orig,tr_pred_orig,test_grad_loss_val,C,True,1e-5,True,M,0.1/((num_tr_sample+1)*C))\n",
    "iv_hvp_perturbed = inverse_hvp_lr_newtonCG(X_train_pert,y_train_pert,tr_pred_pert,test_grad_loss_val,C,True,1e-5,True,M,0.1/((num_tr_sample+1)*C))\n",
    "\n",
    "# IF = -1 * (iv_hvp_perturbed - iv_hvp_orig)\n",
    "# IF = (-np.linalg.norm(IF)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "       -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "       -0., -0., -0., -0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IF = -1 * (iv_hvp_perturbed - iv_hvp_orig)\n",
    "#IF = (-np.linalg.norm(IF)) \n",
    "IF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "45263783c026738a01229a76a997ac4b06d8d9e82a135e1f5eb2b8b0ed62292e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
